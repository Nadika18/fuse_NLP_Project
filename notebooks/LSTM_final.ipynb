{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"updated_text_data.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1=df[df['गुनासो वर्ग']=='वेबसाइट तथा अभिलेख व्यवस्थापन सम्बन्धी ']\n",
    "df_2=df[df['गुनासो वर्ग']=='सोधपुछ, सुझाव, प्रशंसा सम्बन्धी'].sample(2863,replace=True)\n",
    "df_2 = df_2.reset_index(drop=True)\n",
    "df_3=df[df['गुनासो वर्ग']=='कर्मचारी सम्वन्धी '].sample(2863,replace=True)\n",
    "df_3 = df_3.reset_index(drop=True)\n",
    "df_4=df[df['गुनासो वर्ग']=='स्वास्थ्यसँग सम्बन्धी'].sample(2863,replace=True)\n",
    "df_4 = df_4.reset_index(drop=True)\n",
    "df_5=df[df['गुनासो वर्ग']=='अर्थ सबन्धी '].sample(2863,replace=True)\n",
    "df_5 = df_5.reset_index(drop=True)\n",
    "df_6=df[df['गुनासो वर्ग']=='खानेपानी सम्बन्धी '].sample(2863,replace=True)\n",
    "df_6 = df_6.reset_index(drop=True)\n",
    "df_7=df[df['गुनासो वर्ग']=='सूचना तथा  संचार सम्बन्धी'].sample(2863,replace=True)\n",
    "df_7 = df_7.reset_index(drop=True)\n",
    "df_8=df[df['गुनासो वर्ग']=='शान्ति सुरक्षा सम्बन्धी '].sample(2863,replace=True)\n",
    "df_8 = df_8.reset_index(drop=True)\n",
    "df_9=df[df['गुनासो वर्ग']=='प्राकृतिक श्रोत/साधन सम्बन्धी'].sample(2863,replace=True)\n",
    "df_9 =df_9.reset_index(drop=True)\n",
    "df_10=df[df['गुनासो वर्ग']=='लागु पदार्थ सम्बन्धी '].sample(2863,replace=True)\n",
    "df_10 = df_10.reset_index(drop=True)\n",
    "df_11=df[df['गुनासो वर्ग']=='अर्थिक अनियमितता तथा भ्रष्टाचार सम्बन्धी '].sample(2863,replace=True)\n",
    "df_11 = df_11.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=pd.concat([df_1,df_2,df_3,df_4,df_5,df_6,df_7,df_8,df_9,df_10,df_11],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "nepali_stopwords = set(stopwords.words('nepali'))\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('[#\\\\/।(),०-९<<?!,—–’‘:\\u200d]', '', text)\n",
    "    text = text.strip('\"')\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in nepali_stopwords and word.lower() not in english_stopwords]\n",
    "    processed_text = ' '.join(filtered_words)\n",
    "    return processed_text\n",
    "\n",
    "df_final['गुनासो'] = df_final['गुनासो'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Labels\n",
    "le = LabelEncoder()\n",
    "df_final['label_encoded'] = le.fit_transform(df_final['गुनासो वर्ग'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_data, test_data = train_test_split(df_final, test_size=0.2, stratify=df_final['label_encoded'], random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "max_words = 10000\n",
    "max_length = 100\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_data['गुनासो'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_data['गुनासो'])\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['गुनासो'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your pre-trained Nepali Word2Vec model\n",
    "word2vec_model_path = \"../complaints/nepaliW2V_5Million.model\"\n",
    "word2vec_model = Word2Vec.load(word2vec_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding matrix\n",
    "embedding_dim = word2vec_model.vector_size\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < max_words and word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSTM model with Word2Vec embeddings\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(Bidirectional(LSTM(units=100, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(units=50)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=50, activation='relu'))\n",
    "model.add(Dense(units=len(np.unique(df_final['label_encoded'])), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "788/788 [==============================] - 87s 104ms/step - loss: 1.2770 - accuracy: 0.5714 - val_loss: 0.9398 - val_accuracy: 0.6684\n",
      "Epoch 2/30\n",
      "788/788 [==============================] - 82s 104ms/step - loss: 0.9042 - accuracy: 0.6889 - val_loss: 0.8296 - val_accuracy: 0.7154\n",
      "Epoch 3/30\n",
      "788/788 [==============================] - 80s 101ms/step - loss: 0.7907 - accuracy: 0.7237 - val_loss: 0.7809 - val_accuracy: 0.7298\n",
      "Epoch 4/30\n",
      "788/788 [==============================] - 78s 99ms/step - loss: 0.7043 - accuracy: 0.7552 - val_loss: 0.7193 - val_accuracy: 0.7514\n",
      "Epoch 5/30\n",
      "788/788 [==============================] - 79s 100ms/step - loss: 0.6568 - accuracy: 0.7709 - val_loss: 0.6996 - val_accuracy: 0.7598\n",
      "Epoch 6/30\n",
      "788/788 [==============================] - 77s 98ms/step - loss: 0.6114 - accuracy: 0.7840 - val_loss: 0.6919 - val_accuracy: 0.7622\n",
      "Epoch 7/30\n",
      "788/788 [==============================] - 77s 98ms/step - loss: 0.6020 - accuracy: 0.7869 - val_loss: 0.6602 - val_accuracy: 0.7717\n",
      "Epoch 8/30\n",
      "788/788 [==============================] - 78s 99ms/step - loss: 0.5734 - accuracy: 0.7964 - val_loss: 0.6627 - val_accuracy: 0.7754\n",
      "Epoch 9/30\n",
      "788/788 [==============================] - 77s 98ms/step - loss: 0.5532 - accuracy: 0.8012 - val_loss: 0.6677 - val_accuracy: 0.7836\n",
      "Epoch 10/30\n",
      "788/788 [==============================] - 77s 98ms/step - loss: 0.5403 - accuracy: 0.8092 - val_loss: 0.6792 - val_accuracy: 0.7833\n",
      "Epoch 11/30\n",
      "788/788 [==============================] - 77s 98ms/step - loss: 0.5234 - accuracy: 0.8127 - val_loss: 0.6620 - val_accuracy: 0.7898\n",
      "Epoch 12/30\n",
      "788/788 [==============================] - 77s 98ms/step - loss: 0.5109 - accuracy: 0.8177 - val_loss: 0.6588 - val_accuracy: 0.7923\n",
      "Epoch 13/30\n",
      "788/788 [==============================] - 79s 100ms/step - loss: 0.5119 - accuracy: 0.8171 - val_loss: 0.6734 - val_accuracy: 0.7863\n",
      "Epoch 14/30\n",
      "788/788 [==============================] - 77s 98ms/step - loss: 0.4981 - accuracy: 0.8208 - val_loss: 0.6603 - val_accuracy: 0.7965\n",
      "Epoch 15/30\n",
      "788/788 [==============================] - 77s 98ms/step - loss: 0.5061 - accuracy: 0.8194 - val_loss: 0.6638 - val_accuracy: 0.7968\n",
      "Epoch 16/30\n",
      "788/788 [==============================] - 78s 98ms/step - loss: 0.4814 - accuracy: 0.8255 - val_loss: 0.6538 - val_accuracy: 0.7990\n",
      "Epoch 17/30\n",
      "788/788 [==============================] - 77s 98ms/step - loss: 0.4901 - accuracy: 0.8263 - val_loss: 0.6484 - val_accuracy: 0.7971\n",
      "Epoch 18/30\n",
      "788/788 [==============================] - 77s 98ms/step - loss: 0.4727 - accuracy: 0.8300 - val_loss: 0.6748 - val_accuracy: 0.8011\n",
      "Epoch 19/30\n",
      "788/788 [==============================] - 78s 99ms/step - loss: 0.4651 - accuracy: 0.8316 - val_loss: 0.6524 - val_accuracy: 0.8060\n",
      "Epoch 20/30\n",
      "788/788 [==============================] - 77s 97ms/step - loss: 0.4726 - accuracy: 0.8317 - val_loss: 0.6632 - val_accuracy: 0.7970\n",
      "Epoch 21/30\n",
      "788/788 [==============================] - 77s 97ms/step - loss: 0.4686 - accuracy: 0.8334 - val_loss: 0.6826 - val_accuracy: 0.7965\n",
      "Epoch 22/30\n",
      "788/788 [==============================] - 78s 99ms/step - loss: 0.4625 - accuracy: 0.8350 - val_loss: 0.6919 - val_accuracy: 0.8033\n",
      "Epoch 23/30\n",
      "788/788 [==============================] - 77s 97ms/step - loss: 0.4684 - accuracy: 0.8312 - val_loss: 0.6886 - val_accuracy: 0.8011\n",
      "Epoch 24/30\n",
      "788/788 [==============================] - 77s 98ms/step - loss: 0.4633 - accuracy: 0.8342 - val_loss: 0.6816 - val_accuracy: 0.8041\n",
      "Epoch 25/30\n",
      "788/788 [==============================] - 77s 98ms/step - loss: 0.4534 - accuracy: 0.8385 - val_loss: 0.6733 - val_accuracy: 0.8095\n",
      "Epoch 26/30\n",
      "788/788 [==============================] - 78s 99ms/step - loss: 0.4433 - accuracy: 0.8409 - val_loss: 0.7111 - val_accuracy: 0.8016\n",
      "Epoch 27/30\n",
      "788/788 [==============================] - 78s 99ms/step - loss: 0.4611 - accuracy: 0.8369 - val_loss: 0.6780 - val_accuracy: 0.8084\n",
      "Epoch 28/30\n",
      "788/788 [==============================] - 77s 98ms/step - loss: 0.4473 - accuracy: 0.8410 - val_loss: 0.6648 - val_accuracy: 0.8084\n",
      "Epoch 29/30\n",
      "788/788 [==============================] - 78s 99ms/step - loss: 0.4335 - accuracy: 0.8447 - val_loss: 0.6859 - val_accuracy: 0.8109\n",
      "Epoch 30/30\n",
      "788/788 [==============================] - 78s 99ms/step - loss: 0.4306 - accuracy: 0.8454 - val_loss: 0.7030 - val_accuracy: 0.8043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f8a19be1060>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 30\n",
    "model.fit(train_padded, train_data['label_encoded'], epochs=epochs, validation_data=(test_padded, test_data['label_encoded']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197/197 [==============================] - 6s 26ms/step\n",
      "Classification Report:\n",
      "                                           precision    recall  f1-score   support\n",
      "\n",
      "                             अर्थ सबन्धी        0.88      0.79      0.83       572\n",
      "अर्थिक अनियमितता तथा भ्रष्टाचार सम्बन्धी        1.00      1.00      1.00       573\n",
      "                       कर्मचारी सम्वन्धी        0.92      0.79      0.85       573\n",
      "                       खानेपानी सम्बन्धी        0.95      0.72      0.82       573\n",
      "            प्राकृतिक श्रोत/साधन सम्बन्धी       0.98      0.92      0.95       573\n",
      "                    लागु पदार्थ सम्बन्धी        0.99      0.97      0.98       572\n",
      "  वेबसाइट तथा अभिलेख व्यवस्थापन सम्बन्धी        0.33      0.75      0.46       573\n",
      "                 शान्ति सुरक्षा सम्बन्धी        0.91      0.65      0.76       573\n",
      "                सूचना तथा  संचार सम्बन्धी       0.94      0.80      0.86       573\n",
      "          सोधपुछ, सुझाव, प्रशंसा सम्बन्धी       0.77      0.67      0.72       572\n",
      "                    स्वास्थ्यसँग सम्बन्धी       0.89      0.79      0.84       572\n",
      "\n",
      "                                 accuracy                           0.80      6299\n",
      "                                macro avg       0.87      0.80      0.82      6299\n",
      "                             weighted avg       0.87      0.80      0.82      6299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "y_pred_probs = model.predict(test_padded)\n",
    "\n",
    "# Convert predicted probabilities to class labels\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Decode the encoded labels back to original classes\n",
    "y_true = le.inverse_transform(test_data['label_encoded'])\n",
    "y_pred_decoded = le.inverse_transform(y_pred)\n",
    "\n",
    "# Print the classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred_decoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted Class: सोधपुछ, सुझाव, प्रशंसा सम्बन्धी (Class 9)\n"
     ]
    }
   ],
   "source": [
    "# New Nepali text\n",
    "new_text = \"यस महानगरपालिकाको सुनाकोठी नखिपोट नख्खुडोल गोकुल आवास कान्तिपुर कोलोनीका वासिन्दाले दिनरात यो धुंवाको सास कहिलेसम्म फेर्नुपर्ने हो हजुर?\"\n",
    "\n",
    "# Preprocess the new text\n",
    "processed_text = preprocess_text(new_text)\n",
    "\n",
    "# Tokenize and pad the sequence\n",
    "new_sequence = tokenizer.texts_to_sequences([processed_text])\n",
    "new_padded = pad_sequences(new_sequence, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Make predictions\n",
    "new_pred_probs = model.predict(new_padded)\n",
    "new_pred_class = np.argmax(new_pred_probs, axis=1)[0]\n",
    "predicted_class_label = le.inverse_transform([new_pred_class])[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Predicted Class: {predicted_class_label} (Class {new_pred_class})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted Class: सोधपुछ, सुझाव, प्रशंसा सम्बन्धी (Class 9)\n"
     ]
    }
   ],
   "source": [
    "# New Nepali text\n",
    "new_text = \"सुनसरी जिल्ला इटहरी उप महानगर भित्र अवस्थित धरान पुग्ने यो बाटो । कुन ठेकेदारले यो काम गर्दा कति पैसा कुम्ल्याउन पाइने हो र आफ्नो निर्वाचन ताका खर्च भएको करोड उठाउन पाइन्छ भनेर त हैन ?\"\n",
    "\n",
    "# Preprocess the new text\n",
    "processed_text = preprocess_text(new_text)\n",
    "\n",
    "# Tokenize and pad the sequence\n",
    "new_sequence = tokenizer.texts_to_sequences([processed_text])\n",
    "new_padded = pad_sequences(new_sequence, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Make predictions\n",
    "new_pred_probs = model.predict(new_padded)\n",
    "new_pred_class = np.argmax(new_pred_probs, axis=1)[0]\n",
    "predicted_class_label = le.inverse_transform([new_pred_class])[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Predicted Class: {predicted_class_label} (Class {new_pred_class})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted Class: अर्थिक अनियमितता तथा भ्रष्टाचार सम्बन्धी  (Class 1)\n"
     ]
    }
   ],
   "source": [
    "# New Nepali text\n",
    "new_text = \"इटहरीदेखि पूर्वाञ्चल विश्वविद्यालय, माेरङ हुँदै पूर्व जाने बाटाे जुन लालभित्तीमा गएर टुङ्गिन्छ, पिच गर्नुपर्‍याे। बीचमा दुइटा ठूला खाेला छन्, त्यसमा पुल हाल्नुपर्‍याे। याे बाटाेलाई वैकल्पिक राजमार्गकाे रुपमा विकास गर्नुप्रयो।\"\n",
    "\n",
    "# Preprocess the new text\n",
    "processed_text = preprocess_text(new_text)\n",
    "\n",
    "# Tokenize and pad the sequence\n",
    "new_sequence = tokenizer.texts_to_sequences([processed_text])\n",
    "new_padded = pad_sequences(new_sequence, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Make predictions\n",
    "new_pred_probs = model.predict(new_padded)\n",
    "new_pred_class = np.argmax(new_pred_probs, axis=1)[0]\n",
    "predicted_class_label = le.inverse_transform([new_pred_class])[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Predicted Class: {predicted_class_label} (Class {new_pred_class})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted Class: अर्थ सबन्धी  (Class 0)\n"
     ]
    }
   ],
   "source": [
    "# New Nepali text\n",
    "new_text = \"मेयरले उपभोक्ता समितिसँग ३० प्रतिशत कमिसन लिएपछि विकासको काम गुणस्तरहीन, दबाब झेल्न नसकेर प्रशासकीय अधिकृतको भागाभाग। बाह्रबिसेका मेयरको मनोमानी- दोहोरीमा रमाइलो गरेको बिलसमेत नगरपालिकाबाटै भुक्तानी गर्न दबाब मेयरले उपभोक्ता समितिसँग ३० प्रतिशत कमिसन लिएपछि विकासको काम गुणस्तरहीन, दबाब झेल्न नसकेर प्रशासकीय अधिकृतको भागाभाग।\"\n",
    "\n",
    "# Preprocess the new text\n",
    "processed_text = preprocess_text(new_text)\n",
    "\n",
    "# Tokenize and pad the sequence\n",
    "new_sequence = tokenizer.texts_to_sequences([processed_text])\n",
    "new_padded = pad_sequences(new_sequence, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Make predictions\n",
    "new_pred_probs = model.predict(new_padded)\n",
    "new_pred_class = np.argmax(new_pred_probs, axis=1)[0]\n",
    "predicted_class_label = le.inverse_transform([new_pred_class])[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Predicted Class: {predicted_class_label} (Class {new_pred_class})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
